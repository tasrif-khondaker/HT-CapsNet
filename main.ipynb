{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b24df6d",
   "metadata": {},
   "source": [
    "***\n",
    "<font size=\"6\"><center><b>\n",
    "Multi-Level Routing with Hierarchical Capsule Voting\n",
    "</b></center></font>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae9d6d5",
   "metadata": {},
   "source": [
    "# Files and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4aa737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Libraries\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "import csv\n",
    "import numpy as np # type: ignore\n",
    "import pandas as pd # type: ignore\n",
    "import matplotlib # type: ignore\n",
    "import matplotlib.pyplot as plt # type: ignore\n",
    "from datetime import datetime\n",
    "from treelib import Tree # type: ignore\n",
    "import platform\n",
    "\n",
    "import tensorflow as tf # type: ignore\n",
    "from tensorflow import keras # type: ignore\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator # type: ignore\n",
    "from tensorflow.keras.models import Sequential  # type: ignore\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization  # type: ignore\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D  # type: ignore\n",
    "from tensorflow.keras import regularizers, optimizers  # type: ignore\n",
    "from tensorflow.keras import backend as K  # type: ignore\n",
    "    ## Tensorflow_docs\n",
    "import tensorflow_docs as tfdocs # type: ignore\n",
    "import tensorflow_docs.plots # type: ignore\n",
    "# Supporting Libraries:\n",
    "# sys.path.append('../../') ### adding system path for src folder\n",
    "from src import *\n",
    "\n",
    "# Auto reload local libraries if updated (for development in jupyter)\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7423b57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sysenv.systeminfo())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2c610b",
   "metadata": {},
   "source": [
    "# System Arguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b15922c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tuple(arg_string):\n",
    "    # Remove parentheses if they are included in the string\n",
    "    arg_string = arg_string.strip('()')\n",
    "    \n",
    "    # Split the string by commas and convert each element to an integer (or another required type)\n",
    "    return tuple(map(int, arg_string.split(',')))\n",
    "\n",
    "parser = argparse.ArgumentParser(prog='Multi-Level Routing',\n",
    "                                 description='Multi-Level Routing with Hierarchical Capsule Voting')\n",
    "args = argparse.Namespace(description='Multi-Level Routing with Hierarchical Capsule Voting')\n",
    "\n",
    "# System config:\n",
    "parser.add_argument('--seed', default=42, type=int, help='random seed')\n",
    "parser.add_argument('--gpus', default='0', type=str, help='gpu id to use')\n",
    "\n",
    "# Dataset config:\n",
    "parser.add_argument('--dataset', default='CIFAR10', type=str, help='train dataset', choices=[\n",
    "                                                                                        'MNIST',\n",
    "                                                                                        'E_MNIST',\n",
    "                                                                                        'F_MNIST',\n",
    "                                                                                        'CIFAR10',\n",
    "                                                                                        'CIFAR100',\n",
    "                                                                                        'S_Cars',\n",
    "                                                                                        'CU_Birds',\n",
    "                                                                                        'M_Tree',\n",
    "                                                                                        'M_Tree_L4',\n",
    "                                                                                        'M_Tree_L3',\n",
    "                                                                                        'M_Tree_L2',\n",
    "                                                                                        'M_Tree_L1',\n",
    "                                                                                        ])\n",
    "\n",
    "parser.add_argument('--data_path', default=None, type=str, help='train dataset')\n",
    "parser.add_argument('--data_normalize', default='StandardScaler', type=str, help='data normalization', choices=['MinMaxScaler', 'StandardScaler', 'None'])\n",
    "parser.add_argument('--data_aug', default='H_MixUp', type=str, help='data augmentation', choices=[\n",
    "                                                                                                'None', 'MixUp', 'CutMix', \n",
    "                                                                                                'MixupAndCutMix', 'MixupORCutMix',\n",
    "                                                                                                'H_MixUp', 'H_CutMix', \n",
    "                                                                                                'H_MixupAndCutMix', 'H_MixupORCutMix',\n",
    "                                                                                             ]\n",
    "                                                                                             )\n",
    "parser.add_argument('--data_aug_alpha', default=0.2, type=float, help='data augmentation alpha value')\n",
    "parser.add_argument('--input_size', default=(64, 64, 3), type=parse_tuple, help='input image size')\n",
    "\n",
    "\n",
    "# model configs:\n",
    "parser.add_argument('--optimizer', default='adam', type=str, help='optimizer', choices=['adam', 'sgd'])\n",
    "parser.add_argument('--DefaultLrScheduler', action='store_false', help='Use default learning rate scheduler with optimizer')\n",
    "parser.add_argument('--initial_lr', default=0.001, type=float, metavar='LR', help='initial learning rate')\n",
    "parser.add_argument('--lr_decay_rate', default=0.9, type=float, help='learning rate decay factor')\n",
    "parser.add_argument('--lr_decay_exe', default=10, type=int, help='learning rate decay epoch')\n",
    "\n",
    "parser.add_argument('--LossType', default='margin', type=str, help='loss function', choices=['margin', 'crossentropy'])\n",
    "parser.add_argument('--LossWeightType', default='Dynamic', type=str, help='Loss Weight Type', choices=['None', 'Dynamic', 'Static'])\n",
    "\n",
    "parser.add_argument('--metric', default='accuracy', type=str, help='metric', choices=['accuracy','loss'])\n",
    "\n",
    "parser.add_argument('--PCaps_dim', default=8, type=int, help='feature dimension')\n",
    "parser.add_argument('--SCaps_dim', default=16, type=int, help='feature dimension')\n",
    "parser.add_argument('--SCaps_dim_mode', default='same', type=str, help='Change of feature dimension with hierarchical level. Increase or decrease by factor of 2', choices=['same', 'increase', 'decrease'])\n",
    "parser.add_argument('--Routing_N', default=2, type=int, help='number of routing iterations')\n",
    "\n",
    "parser.add_argument('--compile_model', action='store_false', help='compile model')\n",
    "parser.add_argument('--backbone_net', default='custom', type=str, help='backbone network', choices=['custom',*[model for model in dir(keras.applications) if callable(getattr(keras.applications, model))]])\n",
    "parser.add_argument('--backbone_net_weights', default=None, type=str, help='backbone network weights path. imagenet or path', metavar='PATH', choices=['imagenet', 'None'])\n",
    "\n",
    "# FOR HTR_CapsNet\n",
    "parser.add_argument('--HTR_taxonomy_temperature', default=0.5, type=float, help='HTR_CapsNet: Controls the sharpness of sigmoid function for taxonomy')\n",
    "parser.add_argument('--HTR_mask_threshold_high', default=0.9, type=float, help='HTR_CapsNet:  Upper bound for mask values')\n",
    "parser.add_argument('--HTR_mask_threshold_low', default=0.1, type=float, help='HTR_CapsNet: Lower bound for mask values')\n",
    "parser.add_argument('--HTR_mask_temperature', default=0.5, type=float, help='HTR_CapsNet: Temperature for routing softmax (HIG VALUE - more distributed routing; LOW VALUE - more focused routing)')\n",
    "parser.add_argument('--HTR_mask_center', default=0.5, type=float, help='HTR_CapsNet: Temperature for routing softmax (HIG VALUE - more distributed routing; LOW VALUE - more focused routing)')\n",
    "parser.add_argument('--HTR_att_num_heads', default=16, type=int, help='HTR_CapsNet: Number of attention heads')\n",
    "parser.add_argument('--HTR_att_key_dim', default=32, type=int, help='HTR_CapsNet: Key dimension for attention mechanism')\n",
    "\n",
    "# training configs: \n",
    "parser.add_argument('--epochs', default=100, type=int, metavar='epoch', help='number of total epochs to run')\n",
    "parser.add_argument('--batch_size', default=32, type=int, metavar='N', help='mini-batch size (default: 32)')\n",
    "\n",
    "parser.add_argument('--NoEarlyStop', action='store_true', help='Enable early stopping based on R@1')\n",
    "parser.add_argument('--early_stop_tolerance', default=20, type=int, metavar='N', help='Early stop tolerance; number of epochs to wait before stopping')\n",
    "\n",
    "parser.add_argument('--mode', default='BUH_CapsNet', type=str, help='training mode',\n",
    "                    choices=['BUH_CapsNet', \n",
    "                             'HD_CapsNet',\n",
    "                             'ML_CapsNet', \n",
    "                             'HDR_CapsNet', \n",
    "                             'HD_CapsNet_Eff', \n",
    "                             'HTR_CapsNet', \n",
    "                             'HD_CapsNet_EM'])\n",
    "\n",
    "parser.add_argument('--logs', default='logs',type=str, help='log directory file name')\n",
    "parser.add_argument('--Test_only', action='store_true', help='Test the model only')\n",
    "\n",
    "\n",
    "# Hyperparameters:\n",
    "parser.add_argument('--m_plus', default=0.9, type=float, help='Margin Loss')\n",
    "parser.add_argument('--m_minus', default=0.1, type=float, help='Margin Loss')\n",
    "parser.add_argument('--lambda_val', default=0.5, type=float, help='Down-weighting of the loss for absent digit classes')\n",
    "\n",
    "# Directories and Unique ID:\n",
    "parser.add_argument('--dir_uid', default=None, type=str, help='Unique ID for the experiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05705e0",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# This cell is tagged with \"parameters\" in the Jupyter Notebook\n",
    "args_dict = {\n",
    "            'dataset' : 'CIFAR10', # 'MNIST', 'E_MNIST', 'F_MNIST', 'CIFAR10', 'CIFAR100', 'S_Cars', 'CU_Birds', 'M_Tree', 'M_Tree_L4', 'M_Tree_L3', 'M_Tree_L2', 'M_Tree_L1'\n",
    "            'data_aug' : 'MixUp', # 'None', 'MixUp', 'CutMix', 'MixupAndCutMix', 'MixupORCutMix', 'H_MixUp', 'H_CutMix', 'H_MixupAndCutMix', 'H_MixupORCutMix'\n",
    "            'mode' : 'HTR_CapsNet', # 'BUH_CapsNet', 'HD_CapsNet', 'ML_CapsNet', 'HDR_CapsNet', 'HD_CapsNet_Eff', 'HTR_CapsNet', 'HD_CapsNet_EM'\n",
    "            'SCaps_dim' : '32', # '8', '16', '32', '64', '128'\n",
    "            'SCaps_dim_mode' : 'decrease', # 'same', 'increase', 'decrease'\n",
    "            'epochs' : '15', # '100', '200', '300', '400', '500'\n",
    "            'gpu' : '0',\n",
    "            'input_size' : '(64, 64, 3)', # '(28, 28, 1)', '(32, 32, 3)', '(64, 64, 3)', '(128, 128, 3)', '(224, 224, 3)'\n",
    "            'data_path' : 'P:\\ath\\to\\dataset\\folder', # Path to the dataset folder\n",
    "            'dir_uid' : 'Test',\n",
    "            # 'Test_only':'BOOL_FLAG', # 'BOOL_FLAG' Special key for on/off flag in argparse\n",
    "            'HTR_mask_threshold_high' : '0.99', # '0.9', '0.95', '0.99' (HIGH VALUE - more distributed routing; LOW VALUE - more focused routing)\n",
    "            'HTR_mask_threshold_low' : '0.1', # '0.1', '0.05', '0.01' (HIGH VALUE - more distributed routing; LOW VALUE - more focused routing)\n",
    "            'HTR_att_num_heads' : '16', # '4', '8', '16', '32', '64' (Number of attention heads)\n",
    "            'HTR_att_key_dim' : '32', # '8', '16', '32', '64', '128' (Key dimension for attention mechanism)\n",
    "            'Routing_N' : '3', # '8', '16', '32', '64', '128' (Key dimension for attention mechanism)\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a700374",
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(args_dict, str):\n",
    "    args_dict = json.loads(args_dict) # Convert string to dictionary\n",
    "\n",
    "args = parser.parse_args([item for key, value in args_dict.items() for item in ((f'--{key}',) if value == 'BOOL_FLAG' else (f'--{key}', str(value)))])\n",
    "\n",
    "args.__dict__.update({k: None if v == 'None' else v for k, v in args.__dict__.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da06120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU Growth: For dynamic GPU memory allocation\n",
    "sysenv.gpugrowth(gpus = args.gpus).memory_growth()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad98815c",
   "metadata": {},
   "source": [
    "# log directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ace371",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if args.dir_uid is not None and (len(args.dir_uid) > 0) and (args.dir_uid.isspace() == False):\n",
    "    args.log_dir = sysenv.log_dir([args.dataset,\n",
    "                                args.data_aug,\n",
    "                                args.mode,\n",
    "                                f'backbone-{args.backbone_net}-{args.backbone_net_weights}-{args.dir_uid}'                                \n",
    "                                ])\n",
    "else:\n",
    "    args.log_dir = sysenv.log_dir([args.dataset,\n",
    "                                args.data_aug,\n",
    "                                args.mode,\n",
    "                                f'backbone-{args.backbone_net}-{args.backbone_net_weights}',\n",
    "                                ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfde972",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7d3743",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets._load_(args.dataset, args= args,\n",
    "                          batch_size= args.batch_size,\n",
    "                          data_normalizing = args.data_normalize,\n",
    "                          data_augmentation = args.data_aug,\n",
    "                          data_aug_alpha = args.data_aug_alpha,\n",
    "                          image_size = args.input_size,\n",
    "                          )\n",
    "\n",
    "ds_train = dataset.training\n",
    "ds_val = dataset.validation\n",
    "ds_test = dataset.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "954bd637",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Print Dataset Sample ###\n",
    "# datasets.print_hierarchical_ds_sample(ds_train, print_batch_size = 2, show_images= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e5be67",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3acb1cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = keras.callbacks.TensorBoard(os.path.join(args.log_dir,\"tb_logs\",datetime.now().strftime(\"%Y%m%d-%H%M%S\")))\n",
    "CSVLogger = keras.callbacks.CSVLogger(os.path.join(args.log_dir,\"log.csv\"), append=True)\n",
    "CallBacks = [tb, CSVLogger]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc5ea00",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0a123c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECTING LOSS FUNCTION\n",
    "if args.LossType == 'margin':\n",
    "    LossFunction = models.capsnet.MarginLoss(m_plus=args.m_plus, m_minus=args.m_minus, lambda_=args.lambda_val)\n",
    "elif args.LossType == 'crossentropy':\n",
    "    LossFunction = keras.losses.CategoricalCrossentropy()\n",
    "else:\n",
    "    raise ValueError('Invalid LossType')\n",
    "\n",
    "# SELECTING LOSS WEIGHTS\n",
    "lw_modifier = models.dynamic_LW_Modifier(num_classes = dataset.num_classes, directory = args.log_dir)\n",
    "if args.LossWeightType == 'Dynamic':\n",
    "    LW_Value = lw_modifier.values\n",
    "    CallBacks = [*CallBacks, lw_modifier]\n",
    "elif args.LossWeightType == 'Static':\n",
    "    LW_Value = lw_modifier.initial_lw\n",
    "elif args.LossWeightType == 'None':\n",
    "    LW_Value = None\n",
    "else:\n",
    "    raise ValueError('Invalid LossWeightType')\n",
    "\n",
    "\n",
    "# SELECTING OPTIMIZER\n",
    "if args.optimizer == 'adam':\n",
    "    Optimizer = keras.optimizers.Adam()\n",
    "elif args.optimizer == 'sgd':\n",
    "    Optimizer = keras.optimizers.SGD()\n",
    "\n",
    "# SELECTING LEARNING RATE SCHEDULER\n",
    "if not not(args.DefaultLrScheduler):\n",
    "    print('Using Learning Rate Scheduler')\n",
    "    LR_Decay = models.LR_ExponentialDecay(initial_LR = args.initial_lr, start_epoch = args.lr_decay_exe, decay_factor = args.lr_decay_rate)\n",
    "    CallBacks = [*CallBacks, LR_Decay.get_scheduler_callback()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f935c049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compiled_model():\n",
    "    model =  models.get_model(\n",
    "                        model_name = args.mode,\n",
    "                        args = args,\n",
    "                        input_shape = args.input_size,\n",
    "                        num_classes = dataset.num_classes,\n",
    "                        taxonomy = dataset.taxonomy,\n",
    "            )\n",
    "    model.compile(\n",
    "                    optimizer=Optimizer, \n",
    "                    loss={k: LossFunction for k in model.output_names},\n",
    "                    # loss_weights=lw_modifier.values,\n",
    "                    loss_weights=LW_Value,\n",
    "                    metrics={k: args.metric for k in model.output_names},\n",
    "                )\n",
    "    return model\n",
    "\n",
    "if (len(args.gpus.split(','))) > 1:\n",
    "    strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce()) if platform.system() == 'Windows' else tf.distribute.MirroredStrategy()\n",
    "    with strategy.scope():\n",
    "        model = get_compiled_model()\n",
    "else:\n",
    "    model = get_compiled_model()\n",
    "    \n",
    "keras.utils.plot_model(model, to_file = os.path.join(args.log_dir,\"Architecture.png\"), show_shapes=True,expand_nested=True);\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc176a2",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96c63fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = [keras.callbacks.ModelCheckpoint(os.path.join(args.log_dir,f'epoch-best-{i}.weights.h5'),\n",
    "                                             monitor=f'val_{i}_accuracy',\n",
    "                                             save_best_only=True, \n",
    "                                             save_weights_only=True, \n",
    "                                             verbose=1) for i in model.output_names]\n",
    "CallBacks = [*CallBacks, *checkpoint]\n",
    "\n",
    "if not args.NoEarlyStop:\n",
    "    early_stop = keras.callbacks.EarlyStopping(monitor=f'val_{model.output_names[-1]}_accuracy', \n",
    "                                            patience=args.early_stop_tolerance,\n",
    "                                            mode='max',\n",
    "                                            restore_best_weights=True)\n",
    "    CallBacks = [*CallBacks, early_stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d46cfa8",
   "metadata": {},
   "source": [
    "## Train/Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac4a0be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if args.Test_only:\n",
    "    try:\n",
    "        model.load_weights(os.path.join(args.log_dir,f\"epoch-best-{model.output_names[-1]}.weights.h5\"))\n",
    "        print(f'Model Weights Loaded Successfully from {os.path.join(args.log_dir,f\"epoch-best-{model.output_names[-1]}.weights.h5\")}')\n",
    "    except:\n",
    "        raise ValueError('Model Weights not found') \n",
    "else:\n",
    "    print('Training the model from scratch | Will overwrite the files in the log directory')\n",
    "    history = model.fit(ds_train,\n",
    "                        epochs = args.epochs,\n",
    "                        validation_data = ds_val,\n",
    "                        callbacks = CallBacks,\n",
    "                        verbose=1)\n",
    "    \n",
    "    print('Training Completed....')\n",
    "    print(f'loading the best weights from {os.path.join(args.log_dir,f\"epoch-best-{model.output_names[-1]}.weights.h5\")}')\n",
    "    model.load_weights(os.path.join(args.log_dir,f\"epoch-best-{model.output_names[-1]}.weights.h5\"))\n",
    "    \n",
    "    pd.DataFrame(history.history).to_csv(os.path.join(args.log_dir,'training_history.csv'), index=False) # Saving training history\n",
    "\n",
    "    plotter = tfdocs.plots.HistoryPlotter()\n",
    "    accuracy_metrics = [metric for metric in history.history.keys() if not metric.startswith('val') and metric.endswith('_accuracy')]\n",
    "    loss_metrics = [metric for metric in history.history.keys() if not metric.startswith('val') and metric.endswith('_loss')]\n",
    "    for metric in accuracy_metrics:\n",
    "        plotter.plot({metric.split('_accuracy')[0].capitalize(): history}, metric=metric)\n",
    "\n",
    "    # Add a title and limit the y-axis\n",
    "    plt.title(\"Model Accuracy\")\n",
    "    plt.ylim([0, 1])\n",
    "    plt.savefig(os.path.join(args.log_dir,f'Model_Accuracy.png'))\n",
    "    plt.close()\n",
    "\n",
    "    for metric in loss_metrics:\n",
    "        plotter.plot({metric.split('_loss')[0].capitalize(): history}, metric=metric)\n",
    "\n",
    "    # Add a title and limit the y-axis\n",
    "    plt.title(\"Model Loss\")\n",
    "    plt.ylim([0, 1])\n",
    "    plt.savefig(os.path.join(args.log_dir,f'Model_Loss.png'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f08f53b",
   "metadata": {},
   "source": [
    "## Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ed4e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(ds_test, verbose=1)\n",
    "print('\\n'.join([f\"{n+1}. {model.metrics_names[n]} ==> {results[n]}\" for n in range(len(results))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ddbfafcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data, y_true, y_pred = models.predict_from_pipeline(model, ds_test,\n",
    "                                                      return_images=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f34a20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.lvl_wise_metric(y_true=y_true,y_pred=y_pred,savedir=args.log_dir,show_graph=False,show_report=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f34a20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_measurements,consistency,exact_match, get_performance_report = metrics.hmeasurements(y_true, y_pred, dataset.label_tree)\n",
    "\n",
    "get_performance_report = {**{'Dataset': dataset.name,\n",
    "                             'Model': model.name,\n",
    "                             'Total Parameters': model.count_params(),\n",
    "                             'Total Trainable Parameters': sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])},\n",
    "                             **get_performance_report\n",
    "                             } # merging two dictionaries (adding Dataset and Model name)\n",
    "performance_metrics = pd.DataFrame(pd.DataFrame(get_performance_report.values(), get_performance_report.keys(), columns = ['Value']))\n",
    "performance_metrics.to_csv(os.path.join(args.log_dir,'performance_metrics.csv'))\n",
    "print(performance_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01c3be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(args.log_dir,'args.json'), 'w') as fid:\n",
    "    json.dump(args.__dict__, fid, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a931a4f",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95147041",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "321px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
